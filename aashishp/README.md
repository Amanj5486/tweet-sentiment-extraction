# Tweet Sentiment Analysis
## Exploring Word Embedding Concepts

The project started with the exploration of various word embeddings like Word2Vec and Glove.
Understood the why One-Hot encoding was insufficient to store relations between the words and why word embeddings based on relation between words was needed.
Further I also used word2vec pre-trained embeddings to find relationships among words, by finding most similar words and also analyzed the biases present in these embeddings and tried to remove these biases.


## Exploring LSTMs and GRUs

I learned how LSTMs are able to capture long term dependencies in input tokens(which are a part of sentence) used the concept of hidden states and forget and update gates.
In this process I also came to know about language modelling and Machine Translation using LSTMs networks .
Machine Translation uses an encoder and a decoder LSTMs to generate the translation.
This same concept extended further which leads to attention models and transformers which are backbone of BERT language model.(https://arxiv.org/abs/1706.03762)


## Exploring Attention and Transformers

Further exploration of LSTMs led me to explore which use the concept of attention in the decoder layers to capture a effect of input of fixed window size in predicting the output.(https://arxiv.org/abs/1706.03762)
This led to exploration of tranformers models which use multi-head attention to capture the relationships between the neighbouring words.



## Exploration of Actual Tweet Dataset of Kaggle

So the exploration started with an Exploratory Data Analysis of the tweet dataset on Kaggle.com which is hosting the competition.


After EDA I came across a model which was using roBerta language model for modelling the problem as question answering task and using roBerta language model by facebook as the pre-trained language model.(https://www.kaggle.com/cdeotte/tensorflow-roberta-0-705, 
			     https://www.kaggle.com/abhishek/roberta-inference-5-folds )

This led me to exploration of the logic and the theory behind Bert based models and how the output generated by this model can be used to perform different NLP tasks.Theses NLP tasks are summarised towards the end of this paper(https://arxiv.org/pdf/1810.04805.pdf ), which is the official paper introducing bert.



## Understanding BERT

As the roBerta model is based on bert the output is similar to bert and can be used similarly.
The best thing about BERT is that it can be used for a variety of tasks with just few lines of fine-tuning(e.g. stacking fully-connected layers at end and using  softmax)
```
    1. Sentence Pair Classification Tasks
    2. Single Sentence Classification Tasks
    3. Question Answering Tasks
    4. Single Sentence Tagging Tasks
```
